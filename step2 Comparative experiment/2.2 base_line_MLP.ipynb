{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Python 的随机模块\n",
    "    np.random.seed(seed)  # NumPy 的随机模块\n",
    "    torch.manual_seed(seed)  # PyTorch CPU 的随机模块\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU 的随机模块\n",
    "    torch.cuda.manual_seed_all(seed)  # 多GPU时\n",
    "    torch.backends.cudnn.deterministic = True  # 让CUDNN使用确定性算法\n",
    "    torch.backends.cudnn.benchmark = False  # 禁用自动寻找最佳卷积算法\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_from_file(\n",
    "    file_path,\n",
    "    continue_feature_list=None,\n",
    "    cat_feature_list=None,\n",
    "    hidden_feature_list=None,\n",
    "    target_col='shared_class'\n",
    "):\n",
    "    \"\"\"\n",
    "    从文件中读取数据并预处理特征，返回 PyTorch Dataset（特征和标签为张量形式）。\n",
    "\n",
    "    参数：\n",
    "        file_path : str\n",
    "            pkl 文件路径。\n",
    "        continue_feature_list : list of str or None\n",
    "        cat_feature_list : list of str or None\n",
    "        hidden_feature_list : list of str or None\n",
    "        target_col : str\n",
    "        scaler : StandardScaler or None\n",
    "\n",
    "    返回：\n",
    "        dataset : TweetDataset\n",
    "        scaler : StandardScaler\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    feature_parts = []\n",
    "\n",
    "    # 连续特征\n",
    "    if continue_feature_list:\n",
    "        scaler = StandardScaler()\n",
    "        X_cont_scaled = scaler.fit_transform(df[continue_feature_list])\n",
    "        cont_scaled_df = pd.DataFrame(X_cont_scaled, columns=continue_feature_list, index=df.index)\n",
    "        feature_parts.append(cont_scaled_df)\n",
    "\n",
    "    # 类别特征\n",
    "    if cat_feature_list:\n",
    "        df[cat_feature_list] = df[cat_feature_list].astype(str)\n",
    "        cat_encoded_df = pd.get_dummies(df[cat_feature_list], drop_first=False)\n",
    "        feature_parts.append(cat_encoded_df)\n",
    "\n",
    "    # 文本/向量特征\n",
    "    if hidden_feature_list:\n",
    "        def expand_vector_features(df, feature_names):\n",
    "            expanded = []\n",
    "            for col in feature_names:\n",
    "                expanded_cols = pd.DataFrame(df[col].tolist(), \n",
    "                                             index=df.index,\n",
    "                                             columns=[f\"{col}_{i}\" for i in range(len(df[col].iloc[0]))])\n",
    "                expanded.append(expanded_cols)\n",
    "            return pd.concat(expanded, axis=1)\n",
    "        X_hidden = expand_vector_features(df, hidden_feature_list)\n",
    "        feature_parts.append(X_hidden)\n",
    "\n",
    "    \n",
    "    X = pd.concat(feature_parts, axis=1).astype(np.float32).values\n",
    "    y = df[target_col].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    dataset = TweetDataset(X_tensor, y_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_acc = None  # 改为基于准确率判断\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_acc):\n",
    "        # 若没有找到最佳准确率，或当前准确率比最好的准确率提升超过delta，则更新\n",
    "        if self.best_acc is None or val_acc > self.best_acc + self.delta:\n",
    "            self.best_acc = val_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataset: Dataset, batch_size=16):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        val_size = int(0.1 * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size - val_size\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = random_split(self.dataset, [train_size, val_size, test_size])\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def train_and_evaluate(self, num_epochs=50):\n",
    "\n",
    "        self.model = MLP(input_dim =self.dataset.X.shape[1], hidden_dim = 1024, output_dim=4).to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        early_stopper = EarlyStopping(patience=5)\n",
    "        best_val_acc = 0\n",
    "        best_model_wts = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            y_true_train, y_pred_train = [], []\n",
    "\n",
    "            for X_batch, y_batch in self.train_loader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true_train.extend(y_batch.cpu().numpy())\n",
    "                y_pred_train.extend(predicted.cpu().numpy())\n",
    "            # 获得训练集上的准确率\n",
    "            train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "            val_acc = self.evaluate(self.val_loader)\n",
    "            test_acc = self.evaluate(self.test_loader)\n",
    "\n",
    "                            # 早停策略检查\n",
    "            if val_acc > best_val_acc:\n",
    "                best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            scheduler.step(val_acc)  \n",
    "\n",
    "            print(f\"Epoch {epoch} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "            early_stopper(val_acc)\n",
    "            if early_stopper.early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "        self.final_evaluation()\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        y_true_val, y_pred_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true_val.extend(y_batch.cpu().numpy())\n",
    "                y_pred_val.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        return val_acc\n",
    "    \n",
    "\n",
    "    def final_evaluation(self):\n",
    "    \n",
    "        # 最终测试性能输出\n",
    "        self.model.eval()\n",
    "        y_true_final, y_pred_final = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in self.test_loader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true_final.extend(y_batch.cpu().numpy())\n",
    "                y_pred_final.extend(predicted.cpu().numpy())\n",
    "                \n",
    "        print(\"\\nFinal model evaluation on test set:\")\n",
    "        mlp_f1 = f1_score(y_true_final, y_pred_final, average='weighted')\n",
    "        mlp_precision = precision_score(y_true_final, y_pred_final, average='weighted')\n",
    "        mlp_recall = recall_score(y_true_final, y_pred_final, average='weighted')\n",
    "        mlp_accuracy = accuracy_score(y_true_final, y_pred_final)\n",
    "\n",
    "        print(\"\\nMLP:\")\n",
    "        print(f\"F1 Score: {mlp_f1}\")\n",
    "        print(f\"Precision: {mlp_precision}\")\n",
    "        print(f\"Recall: {mlp_recall}\")\n",
    "        print(f\"Accuracy: {mlp_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_features = [ \"create_time\", \"follows\", \"fans\", \"content_len\"]\n",
    "\n",
    "cat_features = [ \"gender\", \"sentiment_class\",\"post_day\", \"post_weekday\", \n",
    "                  \"post_month\", \"post_hour\", \"post_minute\"]\n",
    "\n",
    "hidden_features = [ \"content_wv_embed\", \"desc_wv_embed\", \n",
    "                    \"content_tfidf\", \"desc_tfidf\", \"embedding\"]\n",
    "\n",
    "dataset = prepare_dataset_from_file(\"../data/bert_data.pkl\",continue_features , cat_features, hidden_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Trainer(dataset)\n",
    "train.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Acc: 0.5162 | Val Acc: 0.6144 | Test Acc: 0.6364\n",
      "Current learning rate: 0.000020\n",
      "Epoch 1 | Train Acc: 0.6193 | Val Acc: 0.6523 | Test Acc: 0.6463\n",
      "Current learning rate: 0.000020\n",
      "Epoch 2 | Train Acc: 0.6543 | Val Acc: 0.6649 | Test Acc: 0.6607\n",
      "Current learning rate: 0.000020\n",
      "Epoch 3 | Train Acc: 0.6796 | Val Acc: 0.6712 | Test Acc: 0.6670\n",
      "Current learning rate: 0.000020\n",
      "Epoch 4 | Train Acc: 0.7040 | Val Acc: 0.6928 | Test Acc: 0.6760\n",
      "Current learning rate: 0.000020\n",
      "Epoch 5 | Train Acc: 0.7302 | Val Acc: 0.7099 | Test Acc: 0.6787\n",
      "Current learning rate: 0.000020\n",
      "Epoch 6 | Train Acc: 0.7485 | Val Acc: 0.7027 | Test Acc: 0.6949\n",
      "Current learning rate: 0.000020\n",
      "Epoch 7 | Train Acc: 0.7624 | Val Acc: 0.7279 | Test Acc: 0.7111\n",
      "Current learning rate: 0.000020\n",
      "Epoch 8 | Train Acc: 0.7745 | Val Acc: 0.7270 | Test Acc: 0.7138\n",
      "Current learning rate: 0.000020\n",
      "Epoch 9 | Train Acc: 0.7917 | Val Acc: 0.7270 | Test Acc: 0.7066\n",
      "Current learning rate: 0.000020\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 10 | Train Acc: 0.7991 | Val Acc: 0.7189 | Test Acc: 0.7066\n",
      "Current learning rate: 0.000010\n",
      "Epoch 11 | Train Acc: 0.8183 | Val Acc: 0.7378 | Test Acc: 0.7264\n",
      "Current learning rate: 0.000010\n",
      "Epoch 12 | Train Acc: 0.8290 | Val Acc: 0.7441 | Test Acc: 0.7273\n",
      "Current learning rate: 0.000010\n",
      "Epoch 13 | Train Acc: 0.8352 | Val Acc: 0.7216 | Test Acc: 0.7210\n",
      "Current learning rate: 0.000010\n",
      "Epoch 14 | Train Acc: 0.8389 | Val Acc: 0.7279 | Test Acc: 0.7228\n",
      "Current learning rate: 0.000010\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 15 | Train Acc: 0.8462 | Val Acc: 0.7369 | Test Acc: 0.7300\n",
      "Current learning rate: 0.000005\n",
      "Epoch 16 | Train Acc: 0.8578 | Val Acc: 0.7459 | Test Acc: 0.7327\n",
      "Current learning rate: 0.000005\n",
      "Epoch 17 | Train Acc: 0.8643 | Val Acc: 0.7369 | Test Acc: 0.7273\n",
      "Current learning rate: 0.000005\n",
      "Epoch 18 | Train Acc: 0.8619 | Val Acc: 0.7414 | Test Acc: 0.7399\n",
      "Current learning rate: 0.000005\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 19 | Train Acc: 0.8723 | Val Acc: 0.7414 | Test Acc: 0.7309\n",
      "Current learning rate: 0.000003\n",
      "Epoch 20 | Train Acc: 0.8663 | Val Acc: 0.7279 | Test Acc: 0.7327\n",
      "Current learning rate: 0.000003\n",
      "Epoch 21 | Train Acc: 0.8696 | Val Acc: 0.7477 | Test Acc: 0.7408\n",
      "Current learning rate: 0.000003\n",
      "Epoch 22 | Train Acc: 0.8703 | Val Acc: 0.7333 | Test Acc: 0.7354\n",
      "Current learning rate: 0.000003\n",
      "Epoch 23 | Train Acc: 0.8638 | Val Acc: 0.7360 | Test Acc: 0.7426\n",
      "Current learning rate: 0.000003\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 24 | Train Acc: 0.8764 | Val Acc: 0.7315 | Test Acc: 0.7444\n",
      "Current learning rate: 0.000001\n",
      "Epoch 25 | Train Acc: 0.8761 | Val Acc: 0.7342 | Test Acc: 0.7417\n",
      "Current learning rate: 0.000001\n",
      "Epoch 26 | Train Acc: 0.8797 | Val Acc: 0.7450 | Test Acc: 0.7390\n",
      "Current learning rate: 0.000001\n",
      "Early stopping triggered.\n",
      "\n",
      "Final model evaluation on test set:\n",
      "\n",
      "MLP:\n",
      "F1 Score: 0.7369358334227584\n",
      "Precision: 0.7366534737972121\n",
      "Recall: 0.738973897389739\n",
      "Accuracy: 0.738973897389739\n"
     ]
    }
   ],
   "source": [
    "train.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Acc: 0.5078 | Val Acc: 0.6180 | Test Acc: 0.5977\n",
      "Current learning rate: 0.000020\n",
      "Epoch 1 | Train Acc: 0.6132 | Val Acc: 0.6631 | Test Acc: 0.6427\n",
      "Current learning rate: 0.000020\n",
      "Epoch 2 | Train Acc: 0.6433 | Val Acc: 0.6793 | Test Acc: 0.6589\n",
      "Current learning rate: 0.000020\n",
      "Epoch 3 | Train Acc: 0.6706 | Val Acc: 0.6712 | Test Acc: 0.6652\n",
      "Current learning rate: 0.000020\n",
      "Epoch 4 | Train Acc: 0.6908 | Val Acc: 0.6847 | Test Acc: 0.6832\n",
      "Current learning rate: 0.000020\n",
      "Epoch 5 | Train Acc: 0.7144 | Val Acc: 0.6892 | Test Acc: 0.6868\n",
      "Current learning rate: 0.000020\n",
      "Epoch 6 | Train Acc: 0.7322 | Val Acc: 0.6973 | Test Acc: 0.6985\n",
      "Current learning rate: 0.000020\n",
      "Epoch 7 | Train Acc: 0.7553 | Val Acc: 0.6955 | Test Acc: 0.7093\n",
      "Current learning rate: 0.000020\n",
      "Epoch 8 | Train Acc: 0.7620 | Val Acc: 0.7117 | Test Acc: 0.7030\n",
      "Current learning rate: 0.000020\n",
      "Epoch 9 | Train Acc: 0.7752 | Val Acc: 0.7234 | Test Acc: 0.7219\n",
      "Current learning rate: 0.000020\n",
      "Epoch 10 | Train Acc: 0.7956 | Val Acc: 0.7162 | Test Acc: 0.7210\n",
      "Current learning rate: 0.000020\n",
      "Epoch 11 | Train Acc: 0.8056 | Val Acc: 0.7189 | Test Acc: 0.7219\n",
      "Current learning rate: 0.000020\n",
      "Epoch 12 | Train Acc: 0.8209 | Val Acc: 0.7297 | Test Acc: 0.7345\n",
      "Current learning rate: 0.000020\n",
      "Epoch 13 | Train Acc: 0.8335 | Val Acc: 0.7378 | Test Acc: 0.7300\n",
      "Current learning rate: 0.000020\n",
      "Epoch 14 | Train Acc: 0.8412 | Val Acc: 0.7324 | Test Acc: 0.7336\n",
      "Current learning rate: 0.000020\n",
      "Epoch 15 | Train Acc: 0.8462 | Val Acc: 0.7369 | Test Acc: 0.7228\n",
      "Current learning rate: 0.000020\n",
      "Epoch 16 | Train Acc: 0.8600 | Val Acc: 0.7468 | Test Acc: 0.7264\n",
      "Current learning rate: 0.000020\n",
      "Epoch 17 | Train Acc: 0.8640 | Val Acc: 0.7351 | Test Acc: 0.7300\n",
      "Current learning rate: 0.000020\n",
      "Epoch 18 | Train Acc: 0.8730 | Val Acc: 0.7414 | Test Acc: 0.7291\n",
      "Current learning rate: 0.000020\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 19 | Train Acc: 0.8784 | Val Acc: 0.7351 | Test Acc: 0.7354\n",
      "Current learning rate: 0.000010\n",
      "Epoch 20 | Train Acc: 0.8874 | Val Acc: 0.7405 | Test Acc: 0.7372\n",
      "Current learning rate: 0.000010\n",
      "Epoch 21 | Train Acc: 0.8978 | Val Acc: 0.7333 | Test Acc: 0.7390\n",
      "Current learning rate: 0.000010\n",
      "Early stopping triggered.\n",
      "\n",
      "Final model evaluation on test set:\n",
      "\n",
      "MLP:\n",
      "F1 Score: 0.7367861218847024\n",
      "Precision: 0.738569897900839\n",
      "Recall: 0.738973897389739\n",
      "Accuracy: 0.738973897389739\n"
     ]
    }
   ],
   "source": [
    "# 没有连续特征\n",
    "continue_features = []\n",
    "\n",
    "cat_features = [ \"gender\", \"sentiment_class\",\"post_day\", \"post_weekday\", \n",
    "                  \"post_month\", \"post_hour\", \"post_minute\"]\n",
    "\n",
    "hidden_features = [ \"content_wv_embed\", \"desc_wv_embed\", \n",
    "                    \"content_tfidf\", \"desc_tfidf\", \"embedding\"]\n",
    "\n",
    "dataset = prepare_dataset_from_file(\"../data/bert_data.pkl\",continue_features , cat_features, hidden_features)\n",
    "train = Trainer(dataset)\n",
    "train.prepare_data()\n",
    "train.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Acc: 0.5267 | Val Acc: 0.6252 | Test Acc: 0.6274\n",
      "Current learning rate: 0.000020\n",
      "Epoch 1 | Train Acc: 0.6080 | Val Acc: 0.6586 | Test Acc: 0.6535\n",
      "Current learning rate: 0.000020\n",
      "Epoch 2 | Train Acc: 0.6500 | Val Acc: 0.6721 | Test Acc: 0.6823\n",
      "Current learning rate: 0.000020\n",
      "Epoch 3 | Train Acc: 0.6778 | Val Acc: 0.6865 | Test Acc: 0.6895\n",
      "Current learning rate: 0.000020\n",
      "Epoch 4 | Train Acc: 0.6940 | Val Acc: 0.6973 | Test Acc: 0.7111\n",
      "Current learning rate: 0.000020\n",
      "Epoch 5 | Train Acc: 0.7106 | Val Acc: 0.7036 | Test Acc: 0.7102\n",
      "Current learning rate: 0.000020\n",
      "Epoch 6 | Train Acc: 0.7290 | Val Acc: 0.7090 | Test Acc: 0.7129\n",
      "Current learning rate: 0.000020\n",
      "Epoch 7 | Train Acc: 0.7499 | Val Acc: 0.7207 | Test Acc: 0.7273\n",
      "Current learning rate: 0.000020\n",
      "Epoch 8 | Train Acc: 0.7568 | Val Acc: 0.7324 | Test Acc: 0.7336\n",
      "Current learning rate: 0.000020\n",
      "Epoch 9 | Train Acc: 0.7718 | Val Acc: 0.7351 | Test Acc: 0.7336\n",
      "Current learning rate: 0.000020\n",
      "Epoch 10 | Train Acc: 0.7862 | Val Acc: 0.7270 | Test Acc: 0.7336\n",
      "Current learning rate: 0.000020\n",
      "Epoch 11 | Train Acc: 0.7983 | Val Acc: 0.7279 | Test Acc: 0.7291\n",
      "Current learning rate: 0.000020\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 12 | Train Acc: 0.8100 | Val Acc: 0.7351 | Test Acc: 0.7390\n",
      "Current learning rate: 0.000010\n",
      "Epoch 13 | Train Acc: 0.8183 | Val Acc: 0.7396 | Test Acc: 0.7525\n",
      "Current learning rate: 0.000010\n",
      "Epoch 14 | Train Acc: 0.8274 | Val Acc: 0.7486 | Test Acc: 0.7426\n",
      "Current learning rate: 0.000010\n",
      "Epoch 15 | Train Acc: 0.8302 | Val Acc: 0.7360 | Test Acc: 0.7390\n",
      "Current learning rate: 0.000010\n",
      "Epoch 16 | Train Acc: 0.8415 | Val Acc: 0.7514 | Test Acc: 0.7408\n",
      "Current learning rate: 0.000010\n",
      "Epoch 17 | Train Acc: 0.8434 | Val Acc: 0.7568 | Test Acc: 0.7462\n",
      "Current learning rate: 0.000010\n",
      "Epoch 18 | Train Acc: 0.8515 | Val Acc: 0.7450 | Test Acc: 0.7408\n",
      "Current learning rate: 0.000010\n",
      "Epoch 19 | Train Acc: 0.8549 | Val Acc: 0.7468 | Test Acc: 0.7363\n",
      "Current learning rate: 0.000010\n",
      "Epoch 20 | Train Acc: 0.8575 | Val Acc: 0.7586 | Test Acc: 0.7408\n",
      "Current learning rate: 0.000010\n",
      "Epoch 21 | Train Acc: 0.8539 | Val Acc: 0.7586 | Test Acc: 0.7417\n",
      "Current learning rate: 0.000010\n",
      "Epoch 22 | Train Acc: 0.8584 | Val Acc: 0.7559 | Test Acc: 0.7471\n",
      "Current learning rate: 0.000010\n",
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 23 | Train Acc: 0.8664 | Val Acc: 0.7577 | Test Acc: 0.7543\n",
      "Current learning rate: 0.000005\n",
      "Epoch 24 | Train Acc: 0.8730 | Val Acc: 0.7667 | Test Acc: 0.7552\n",
      "Current learning rate: 0.000005\n",
      "Epoch 25 | Train Acc: 0.8759 | Val Acc: 0.7649 | Test Acc: 0.7552\n",
      "Current learning rate: 0.000005\n",
      "Epoch 26 | Train Acc: 0.8752 | Val Acc: 0.7595 | Test Acc: 0.7471\n",
      "Current learning rate: 0.000005\n",
      "Epoch 00028: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 27 | Train Acc: 0.8782 | Val Acc: 0.7541 | Test Acc: 0.7354\n",
      "Current learning rate: 0.000003\n",
      "Epoch 28 | Train Acc: 0.8812 | Val Acc: 0.7550 | Test Acc: 0.7381\n",
      "Current learning rate: 0.000003\n",
      "Epoch 29 | Train Acc: 0.8857 | Val Acc: 0.7631 | Test Acc: 0.7435\n",
      "Current learning rate: 0.000003\n",
      "Early stopping triggered.\n",
      "\n",
      "Final model evaluation on test set:\n",
      "\n",
      "MLP:\n",
      "F1 Score: 0.7422584348857123\n",
      "Precision: 0.7417651571789454\n",
      "Recall: 0.7434743474347435\n",
      "Accuracy: 0.7434743474347435\n"
     ]
    }
   ],
   "source": [
    "# 没有文本特征\n",
    "continue_features = [\"create_time\", \"follows\", \"fans\", \"content_len\"]\n",
    "\n",
    "cat_features = []\n",
    "\n",
    "\n",
    "hidden_features = [ \"content_wv_embed\", \"desc_wv_embed\", \n",
    "                    \"content_tfidf\", \"desc_tfidf\", \"embedding\"]\n",
    "\n",
    "dataset = prepare_dataset_from_file(\"../data/bert_data.pkl\",continue_features , cat_features, hidden_features)\n",
    "train = Trainer(dataset)\n",
    "train.prepare_data()\n",
    "train.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Acc: 0.4656 | Val Acc: 0.5378 | Test Acc: 0.5554\n",
      "Current learning rate: 0.000020\n",
      "Epoch 1 | Train Acc: 0.5155 | Val Acc: 0.5667 | Test Acc: 0.5635\n",
      "Current learning rate: 0.000020\n",
      "Epoch 2 | Train Acc: 0.5318 | Val Acc: 0.5432 | Test Acc: 0.5473\n",
      "Current learning rate: 0.000020\n",
      "Epoch 3 | Train Acc: 0.5508 | Val Acc: 0.5739 | Test Acc: 0.5770\n",
      "Current learning rate: 0.000020\n",
      "Epoch 4 | Train Acc: 0.5639 | Val Acc: 0.5676 | Test Acc: 0.5761\n",
      "Current learning rate: 0.000020\n",
      "Epoch 5 | Train Acc: 0.5844 | Val Acc: 0.5739 | Test Acc: 0.5770\n",
      "Current learning rate: 0.000020\n",
      "Epoch 6 | Train Acc: 0.5854 | Val Acc: 0.5892 | Test Acc: 0.5923\n",
      "Current learning rate: 0.000020\n",
      "Epoch 7 | Train Acc: 0.5901 | Val Acc: 0.5829 | Test Acc: 0.5905\n",
      "Current learning rate: 0.000020\n",
      "Epoch 8 | Train Acc: 0.6041 | Val Acc: 0.5865 | Test Acc: 0.5851\n",
      "Current learning rate: 0.000020\n",
      "Epoch 9 | Train Acc: 0.6214 | Val Acc: 0.5964 | Test Acc: 0.6139\n",
      "Current learning rate: 0.000020\n",
      "Epoch 10 | Train Acc: 0.6270 | Val Acc: 0.6009 | Test Acc: 0.6211\n",
      "Current learning rate: 0.000020\n",
      "Epoch 11 | Train Acc: 0.6333 | Val Acc: 0.6144 | Test Acc: 0.6121\n",
      "Current learning rate: 0.000020\n",
      "Epoch 12 | Train Acc: 0.6358 | Val Acc: 0.6090 | Test Acc: 0.6247\n",
      "Current learning rate: 0.000020\n",
      "Epoch 13 | Train Acc: 0.6426 | Val Acc: 0.6144 | Test Acc: 0.6193\n",
      "Current learning rate: 0.000020\n",
      "Epoch 14 | Train Acc: 0.6521 | Val Acc: 0.6153 | Test Acc: 0.6247\n",
      "Current learning rate: 0.000020\n",
      "Epoch 15 | Train Acc: 0.6610 | Val Acc: 0.6171 | Test Acc: 0.6238\n",
      "Current learning rate: 0.000020\n",
      "Epoch 16 | Train Acc: 0.6630 | Val Acc: 0.6288 | Test Acc: 0.6211\n",
      "Current learning rate: 0.000020\n",
      "Epoch 17 | Train Acc: 0.6716 | Val Acc: 0.6306 | Test Acc: 0.6256\n",
      "Current learning rate: 0.000020\n",
      "Epoch 18 | Train Acc: 0.6774 | Val Acc: 0.6333 | Test Acc: 0.6310\n",
      "Current learning rate: 0.000020\n",
      "Epoch 19 | Train Acc: 0.6822 | Val Acc: 0.6324 | Test Acc: 0.6382\n",
      "Current learning rate: 0.000020\n",
      "Epoch 20 | Train Acc: 0.6832 | Val Acc: 0.6369 | Test Acc: 0.6391\n",
      "Current learning rate: 0.000020\n",
      "Epoch 21 | Train Acc: 0.6930 | Val Acc: 0.6396 | Test Acc: 0.6337\n",
      "Current learning rate: 0.000020\n",
      "Epoch 22 | Train Acc: 0.6977 | Val Acc: 0.6468 | Test Acc: 0.6418\n",
      "Current learning rate: 0.000020\n",
      "Epoch 23 | Train Acc: 0.7002 | Val Acc: 0.6324 | Test Acc: 0.6283\n",
      "Current learning rate: 0.000020\n",
      "Epoch 24 | Train Acc: 0.7102 | Val Acc: 0.6306 | Test Acc: 0.6274\n",
      "Current learning rate: 0.000020\n",
      "Epoch 25 | Train Acc: 0.7180 | Val Acc: 0.6505 | Test Acc: 0.6562\n",
      "Current learning rate: 0.000020\n",
      "Epoch 26 | Train Acc: 0.7157 | Val Acc: 0.6315 | Test Acc: 0.6391\n",
      "Current learning rate: 0.000020\n",
      "Epoch 27 | Train Acc: 0.7199 | Val Acc: 0.6604 | Test Acc: 0.6598\n",
      "Current learning rate: 0.000020\n",
      "Epoch 28 | Train Acc: 0.7267 | Val Acc: 0.6468 | Test Acc: 0.6607\n",
      "Current learning rate: 0.000020\n",
      "Epoch 29 | Train Acc: 0.7312 | Val Acc: 0.6495 | Test Acc: 0.6553\n",
      "Current learning rate: 0.000020\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 30 | Train Acc: 0.7403 | Val Acc: 0.6450 | Test Acc: 0.6580\n",
      "Current learning rate: 0.000010\n",
      "Epoch 31 | Train Acc: 0.7419 | Val Acc: 0.6667 | Test Acc: 0.6706\n",
      "Current learning rate: 0.000010\n",
      "Epoch 32 | Train Acc: 0.7443 | Val Acc: 0.6468 | Test Acc: 0.6598\n",
      "Current learning rate: 0.000010\n",
      "Epoch 33 | Train Acc: 0.7487 | Val Acc: 0.6595 | Test Acc: 0.6625\n",
      "Current learning rate: 0.000010\n",
      "Epoch 00035: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 34 | Train Acc: 0.7594 | Val Acc: 0.6604 | Test Acc: 0.6697\n",
      "Current learning rate: 0.000005\n",
      "Epoch 35 | Train Acc: 0.7656 | Val Acc: 0.6568 | Test Acc: 0.6706\n",
      "Current learning rate: 0.000005\n",
      "Epoch 36 | Train Acc: 0.7524 | Val Acc: 0.6577 | Test Acc: 0.6751\n",
      "Current learning rate: 0.000005\n",
      "Early stopping triggered.\n",
      "\n",
      "Final model evaluation on test set:\n",
      "\n",
      "MLP:\n",
      "F1 Score: 0.6735427026542095\n",
      "Precision: 0.676513524893573\n",
      "Recall: 0.6750675067506751\n",
      "Accuracy: 0.6750675067506751\n"
     ]
    }
   ],
   "source": [
    "#  没有文本特征\n",
    "continue_features = [\"create_time\", \"follows\", \"fans\", \"content_len\"]\n",
    "\n",
    "cat_features = [ \"gender\", \"sentiment_class\",\"post_day\", \"post_weekday\", \n",
    "                  \"post_month\", \"post_hour\", \"post_minute\"]\n",
    "\n",
    "hidden_features = []\n",
    "\n",
    "dataset = prepare_dataset_from_file(\"../data/bert_data.pkl\",continue_features , cat_features, hidden_features)\n",
    "train = Trainer(dataset)\n",
    "train.prepare_data()\n",
    "train.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trasformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
